"""
‚ïî‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïó
‚ïë                   1SKIN PARSER - URL ARTICLE SCRAPER                       ‚ïë
‚ïë                                                                            ‚ïë
‚ïë  –ü–∞—Ä—Å–µ—Ä –¥–ª—è —Å–±–æ—Ä–∞ –¥–∞–Ω–Ω—ã—Ö —Å—Ç–∞—Ç–µ–π —Å–∞–π—Ç–∞ 1skin.ru                           ‚ïë
‚ïë  –°–æ–±–∏—Ä–∞–µ—Ç: URL, –¥–∞—Ç—É –ø—É–±–ª–∏–∫–∞—Ü–∏–∏, –¥–∞—Ç—É –æ–±–Ω–æ–≤–ª–µ–Ω–∏—è, –ø—Ä–æ—Å–º–æ—Ç—Ä—ã, –≤—Ä–µ–º—è —á—Ç–µ–Ω–∏—è‚ïë
‚ïë                                                                            ‚ïë
‚ïë  –ü–û–î–î–ï–†–ñ–ò–í–ê–ï–ú–´–ï –†–ê–ó–î–ï–õ–´:                                                  ‚ïë
‚ïë  ‚Ä¢ /article/ - –æ—Å–Ω–æ–≤–Ω—ã–µ —Å—Ç–∞—Ç—å–∏                                            ‚ïë
‚ïë  ‚Ä¢ /want/    - —Å—Ç–∞—Ç—å–∏ –æ –∂–µ–ª–∞–µ–º–æ–º (—Å –≤–µ—Ä—Å–∏–∏ 2.1)                           ‚ïë
‚ïë                                                                            ‚ïë
‚ïë  –ò–°–ü–û–õ–¨–ó–û–í–ê–ù–ò–ï:                                                            ‚ïë
‚ïë  1. –£—Å—Ç–∞–Ω–æ–≤–∏ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏: pip install requests beautifulsoup4 pandas      ‚ïë
‚ïë     openpyxl lxml                                                         ‚ïë
‚ïë  2. –°–æ–∑–¥–∞–π —Ñ–∞–π–ª urls.txt —Ä—è–¥–æ–º —Å–æ —Å–∫—Ä–∏–ø—Ç–æ–º                               ‚ïë
‚ïë  3. –î–æ–±–∞–≤—å —Ç—É–¥–∞ URL (–∫–∞–∂–¥—ã–π –Ω–∞ –Ω–æ–≤–æ–π —Å—Ç—Ä–æ–∫–µ)                             ‚ïë
‚ïë  4. –ó–∞–ø—É—Å—Ç–∏: python 1skin_parser_url_article.py                          ‚ïë
‚ïë                                                                            ‚ïë
‚ïë  –†–ï–ó–£–õ–¨–¢–ê–¢:                                                                ‚ïë
‚ïë  ‚úì –§–∞–π–ª 1skin_articles.xlsx —Å –ø–æ–ª–Ω–æ–π —Ç–∞–±–ª–∏—Ü–µ–π                            ‚ïë
‚ïë  ‚úì –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –≤ –∫–æ–Ω—Å–æ–ª–∏                                                   ‚ïë
‚ïë  ‚úì –ü—Ä–∏–º–µ—Ä—ã –¥–∞–Ω–Ω—ã—Ö                                                          ‚ïë
‚ïö‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïù
"""

import requests
from bs4 import BeautifulSoup
import pandas as pd
import time
import re
import os
import json
from pathlib import Path


class SkinArticleScraper:
    """–ü–∞—Ä—Å–µ—Ä —Å—Ç–∞—Ç–µ–π —Å–∞–π—Ç–∞ 1skin.ru —Å –ø–æ–¥–¥–µ—Ä–∂–∫–æ–π /article/ –∏ /want/"""
    
    def __init__(self, urls_file='urls.txt'):
        """
        –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –ø–∞—Ä—Å–µ—Ä–∞
        
        Args:
            urls_file (str): –ò–º—è —Ñ–∞–π–ª–∞ —Å URL —Å—Ç–∞—Ç–µ–π
        """
        self.base_url = "https://1skin.ru"
        self.session = requests.Session()
        self.session.headers.update({
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'
        })
        self.articles = []
        script_dir = Path(__file__).parent.absolute()
        self.urls_file = script_dir / urls_file
    
    def load_urls_from_file(self):
        """–ó–∞–≥—Ä—É–∑–∏—Ç—å —Å–ø–∏—Å–æ–∫ URL —Å—Ç–∞—Ç–µ–π –∏–∑ —Ç–µ–∫—Å—Ç–æ–≤–æ–≥–æ —Ñ–∞–π–ª–∞"""
        try:
            if not self.urls_file.exists():
                print(f"‚úó –§–∞–π–ª –Ω–µ –Ω–∞–π–¥–µ–Ω: {self.urls_file}")
                print(f"\nüìÅ –¢–µ–∫—É—â–∞—è —Ä–∞–±–æ—á–∞—è –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—è: {os.getcwd()}")
                print(f"üìÅ –î–∏—Ä–µ–∫—Ç–æ—Ä–∏—è —Å–∫—Ä–∏–ø—Ç–∞: {self.urls_file.parent}")
                print(f"\nüí° –ü–æ–∂–∞–ª—É–π—Å—Ç–∞:")
                print(f"  1. –°–æ–∑–¥–∞–π —Ñ–∞–π–ª 'urls.txt' –≤ –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–∏: {self.urls_file.parent}")
                print(f"  2. –î–æ–±–∞–≤—å —Ç—É–¥–∞ URL —Å—Ç–∞—Ç–µ–π (–∫–∞–∂–¥—ã–π –Ω–∞ –Ω–æ–≤–æ–π —Å—Ç—Ä–æ–∫–µ)")
                print(f"  3. –ó–∞–ø—É—Å—Ç–∏ —Å–∫—Ä–∏–ø—Ç —Å–Ω–æ–≤–∞\n")
                
                print(f"üìã –§–∞–π–ª—ã –≤ –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–∏ {self.urls_file.parent}:")
                for file in self.urls_file.parent.iterdir():
                    if file.is_file():
                        print(f"   - {file.name} ({file.stat().st_size} –±–∞–π—Ç)")
                
                return []
            
            with open(self.urls_file, 'r', encoding='utf-8') as f:
                urls = [line.strip() for line in f if line.strip()]
            
            print(f"‚úì –ó–∞–≥—Ä—É–∂–µ–Ω–æ {len(urls)} URL –∏–∑ —Ñ–∞–π–ª–∞ {self.urls_file.name}")
            return urls
        
        except Exception as e:
            print(f"‚úó –û—à–∏–±–∫–∞ –ø—Ä–∏ —á—Ç–µ–Ω–∏–∏ —Ñ–∞–π–ª–∞: {e}")
            return []
    
    def get_sitemap_urls(self):
        """–ü–æ–ª—É—á–∏—Ç—å —Å–ø–∏—Å–æ–∫ –≤—Å–µ—Ö URL —Å—Ç–∞—Ç–µ–π –∏–∑ sitemap.xml —Å–∞–π—Ç–∞"""
        try:
            sitemap_url = f"{self.base_url}/sitemap.xml"
            response = self.session.get(sitemap_url, timeout=10)
            soup = BeautifulSoup(response.content, 'xml')
            
            # –ò—â–µ–º URL –∏–∑ –æ–±–æ–∏—Ö —Ä–∞–∑–¥–µ–ª–æ–≤: /article/ –∏ /want/
            urls = [loc.text for loc in soup.find_all('loc') 
                   if '/article/' in loc.text or '/want/' in loc.text]
            
            print(f"‚úì –ù–∞–π–¥–µ–Ω–æ {len(urls)} —Å—Ç–∞—Ç–µ–π –≤ sitemap")
            return urls
        
        except Exception as e:
            print(f"‚úó –û—à–∏–±–∫–∞ –ø—Ä–∏ –ø–∞—Ä—Å–∏–Ω–≥–µ sitemap: {e}")
            return []
    
    def extract_article_data(self, url):
        """–ó–∞–≥—Ä—É–∑–∏—Ç—å –∏ —Å–ø–∞—Ä—Å–∏—Ç—å –¥–∞–Ω–Ω—ã–µ –æ–¥–Ω–æ–π —Å—Ç–∞—Ç—å–∏"""
        try:
            response = self.session.get(url, timeout=10)
            response.raise_for_status()
            soup = BeautifulSoup(response.content, 'html.parser')
            
            # –ò–∑–≤–ª–µ–∫–∞–µ–º —á–µ—Ç—ã—Ä–µ –∑–Ω–∞—á–µ–Ω–∏—è
            publish_date = self._get_publish_date(soup)
            update_date = self._get_update_date(soup)
            views = self._get_views_count(soup)
            read_time = self._get_read_time(soup)
            
            return {
                'URL —Å—Ç—Ä–∞–Ω–∏—Ü—ã': url,
                '–î–∞—Ç–∞ –ø—É–±–ª–∏–∫–∞—Ü–∏–∏': publish_date,
                '–î–∞—Ç–∞ –æ–±–Ω–æ–≤–ª–µ–Ω–∏—è': update_date,
                '–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –ø—Ä–æ—Å–º–æ—Ç—Ä–æ–≤': views,
                '–í—Ä–µ–º—è —á—Ç–µ–Ω–∏—è (–º–∏–Ω)': read_time
            }
        
        except Exception as e:
            return None
    
    def _get_publish_date(self, soup):
        """
        –ò–∑–≤–ª–µ—á—å –¥–∞—Ç—É –ø—É–±–ª–∏–∫–∞—Ü–∏–∏ —Å—Ç–∞—Ç—å–∏
        –°—Ç—Ä—É–∫—Ç—É—Ä–∞: "–î–∞—Ç–∞ –ø—É–±–ª–∏–∫–∞—Ü–∏–∏: 14.01.2019"
        """
        try:
            # –ú–ï–¢–û–î 1: –ò—â–µ–º —Ç–µ–∫—Å—Ç "–î–∞—Ç–∞ –ø—É–±–ª–∏–∫–∞—Ü–∏–∏:" –≤ —Å—Ç—Ä–∞–Ω–∏—Ü–µ
            text = soup.get_text()
            match = re.search(r'–î–∞—Ç–∞\s+–ø—É–±–ª–∏–∫–∞—Ü–∏–∏[:\s]+(\d{1,2}\.\d{1,2}\.\d{4})', text, re.IGNORECASE)
            if match:
                return match.group(1)
            
            # –ú–ï–¢–û–î 2: –ò—â–µ–º —á–µ—Ä–µ–∑ JSON-LD —Å—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ
            scripts = soup.find_all('script', {'type': 'application/ld+json'})
            for script in scripts:
                try:
                    data = json.loads(script.string)
                    if isinstance(data, dict):
                        if 'datePublished' in data:
                            date_str = data['datePublished'][:10]
                            date_obj = __import__('datetime').datetime.strptime(date_str, '%Y-%m-%d')
                            return date_obj.strftime('%d.%m.%Y')
                except:
                    pass
            
            # –ú–ï–¢–û–î 3: –ò—â–µ–º –≤ section —Å –∫–ª–∞—Å—Å–æ–º section-purple
            section = soup.find('section', class_='section-purple')
            if section:
                divs = section.find_all('div', class_='pt-3')
                if divs:
                    pt3_div = divs[0]
                    date_div = pt3_div.find('div')
                    if date_div:
                        date_text = date_div.get_text(strip=True)
                        if re.match(r'\d{1,2}\.\d{1,2}\.\d{4}', date_text):
                            return date_text
            
            # –ú–ï–¢–û–î 4: –ò—â–µ–º –ª—é–±—É—é –¥–∞—Ç—É –≤ —Ñ–æ—Ä–º–∞—Ç–µ DD.MM.YYYY –≤ –Ω–∞—á–∞–ª–µ —Å—Ç—Ä–∞–Ω–∏—Ü—ã
            dates = re.findall(r'\d{1,2}\.\d{1,2}\.\d{4}', text[:2000])
            if dates:
                return dates[0]
            
            return '–ù–µ –Ω–∞–π–¥–µ–Ω–∞'
        
        except Exception as e:
            return '–ù–µ –Ω–∞–π–¥–µ–Ω–∞'
    
    def _get_update_date(self, soup):
        """
        –ò–∑–≤–ª–µ—á—å –¥–∞—Ç—É –æ–±–Ω–æ–≤–ª–µ–Ω–∏—è —Å—Ç–∞—Ç—å–∏
        –°—Ç—Ä—É–∫—Ç—É—Ä–∞: "–î–∞—Ç–∞ –æ–±–Ω–æ–≤–ª–µ–Ω–∏—è: 03.06.2025"
        –≠—Ç–æ –ø–æ–ª–µ –º–æ–∂–µ—Ç –±—ã—Ç—å –æ—Ç—Å—É—Ç—Å—Ç–≤–æ–≤–∞—Ç—å –Ω–∞ —Å—Ç–∞—Ä—ã—Ö —Å—Ç–∞—Ç—å—è—Ö
        """
        try:
            text = soup.get_text()
            
            # –ú–ï–¢–û–î 1: –ò—â–µ–º —Ç–µ–∫—Å—Ç "–î–∞—Ç–∞ –æ–±–Ω–æ–≤–ª–µ–Ω–∏—è:" –≤ —Å—Ç—Ä–∞–Ω–∏—Ü–µ
            match = re.search(r'–î–∞—Ç–∞\s+–æ–±–Ω–æ–≤–ª–µ–Ω–∏—è[:\s]+(\d{1,2}\.\d{1,2}\.\d{4})', text, re.IGNORECASE)
            if match:
                return match.group(1)
            
            # –ú–ï–¢–û–î 2: –ò—â–µ–º —á–µ—Ä–µ–∑ JSON-LD —Å—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ (dateModified)
            scripts = soup.find_all('script', {'type': 'application/ld+json'})
            for script in scripts:
                try:
                    data = json.loads(script.string)
                    if isinstance(data, dict):
                        if 'dateModified' in data:
                            date_str = data['dateModified'][:10]
                            date_obj = __import__('datetime').datetime.strptime(date_str, '%Y-%m-%d')
                            return date_obj.strftime('%d.%m.%Y')
                except:
                    pass
            
            # –ú–ï–¢–û–î 3: –ò—â–µ–º –≤ section —Å –∫–ª–∞—Å—Å–æ–º section-purple (–≤—Ç–æ—Ä–∞—è –¥–∞—Ç–∞ –≤ pt-3)
            # –°—Ç—Ä—É–∫—Ç—É—Ä–∞ –º–æ–∂–µ—Ç –±—ã—Ç—å: [–¥–∞—Ç–∞ –ø—É–±–ª–∏–∫–∞—Ü–∏–∏, –¥–∞—Ç–∞ –æ–±–Ω–æ–≤–ª–µ–Ω–∏—è, –≤—Ä–µ–º—è —á—Ç–µ–Ω–∏—è]
            section = soup.find('section', class_='section-purple')
            if section:
                divs = section.find_all('div', class_='pt-3')
                if divs:
                    pt3_div = divs[0]
                    child_divs = pt3_div.find_all('div', recursive=False)
                    
                    # –ï—Å–ª–∏ –µ—Å—Ç—å –±–æ–ª—å—à–µ —á–µ–º –æ–¥–Ω–∞ –¥–∞—Ç–∞, –≤—Ç–æ—Ä–∞—è - —ç—Ç–æ –¥–∞—Ç–∞ –æ–±–Ω–æ–≤–ª–µ–Ω–∏—è
                    if len(child_divs) >= 2:
                        # –ü—Ä–æ–≤–µ—Ä—è–µ–º, —ç—Ç–æ –ª–∏ –≤—Ç–æ—Ä–∞—è –¥–∞—Ç–∞
                        second_text = child_divs[1].get_text(strip=True)
                        if re.match(r'\d{1,2}\.\d{1,2}\.\d{4}', second_text):
                            return second_text
            
            return '–ù–µ —É–∫–∞–∑–∞–Ω–∞'
        
        except Exception as e:
            return '–ù–µ —É–∫–∞–∑–∞–Ω–∞'
    
    def _get_views_count(self, soup):
        """
        –ò–∑–≤–ª–µ—á—å –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –ø—Ä–æ—Å–º–æ—Ç—Ä–æ–≤ —Å—Ç–∞—Ç—å–∏
        –°—Ç—Ä—É–∫—Ç—É—Ä–∞: "–ü—Ä–æ—Å–º–æ—Ç—Ä–µ–Ω–æ: 135"
        """
        try:
            text = soup.get_text()
            
            # –ú–ï–¢–û–î 1: –ò—â–µ–º —Ç–µ–∫—Å—Ç "–ü—Ä–æ—Å–º–æ—Ç—Ä–µ–Ω–æ:" –∏–ª–∏ "–ü—Ä–æ—Å–º–æ—Ç—Ä–æ"
            match = re.search(r'–ü—Ä–æ—Å–º–æ—Ç—Ä–µ–Ω–æ[:\s]+(\d+)', text, re.IGNORECASE)
            if match:
                return int(match.group(1))
            
            # –ú–ï–¢–û–î 2: –ò—â–µ–º —á–µ—Ä–µ–∑ meta —Ç–µ–≥–∏
            meta_views = soup.find('meta', {'property': 'article:view_count'})
            if meta_views:
                views = meta_views.get('content', '')
                if views.isdigit():
                    return int(views)
            
            # –ú–ï–¢–û–î 3: –ò—â–µ–º –≤ —Ä–∞–∑–Ω—ã—Ö data –∞—Ç—Ä–∏–±—É—Ç–∞—Ö
            for elem in soup.find_all(['div', 'span']):
                if elem.get('data-views'):
                    views = re.findall(r'\d+', str(elem.get('data-views')))
                    if views:
                        return int(views[0])
                
                classes = elem.get('class', [])
                if any('view' in c.lower() for c in classes if isinstance(c, str)):
                    numbers = re.findall(r'\d+', elem.get_text())
                    if numbers:
                        return int(numbers[0])
            
            # –ú–ï–¢–û–î 4: –ò—Å—Ö–æ–¥–Ω—ã–π –º–µ—Ç–æ–¥ - —á–µ—Ä–µ–∑ pt-3 —Å–µ–ª–µ–∫—Ç–æ—Ä (—Ç—Ä–µ—Ç—å—è –ø–æ–∑–∏—Ü–∏—è)
            section = soup.find('section', class_='section-purple')
            if section:
                divs = section.find_all('div', class_='pt-3')
                if divs:
                    pt3_div = divs[0]
                    child_divs = pt3_div.find_all('div', recursive=False)
                    if len(child_divs) >= 3:  # 3-—è –ø–æ–∑–∏—Ü–∏—è –¥–ª—è –ø—Ä–æ—Å–º–æ—Ç—Ä–æ–≤
                        views_text = child_divs[2].get_text(strip=True)
                        numbers = re.findall(r'\d+', views_text)
                        if numbers:
                            return int(numbers[0])
            
            return '–ù–µ —É–∫–∞–∑–∞–Ω–æ'
        
        except Exception as e:
            return '–ù–µ —É–∫–∞–∑–∞–Ω–æ'
    
    def _get_read_time(self, soup):
        """
        –ò–∑–≤–ª–µ—á—å –≤—Ä–µ–º—è —á—Ç–µ–Ω–∏—è —Å—Ç–∞—Ç—å–∏ –≤ –º–∏–Ω—É—Ç–∞—Ö
        –°—Ç—Ä—É–∫—Ç—É—Ä–∞: "–í—Ä–µ–º—è —á—Ç–µ–Ω–∏—è: 18 –º–∏–Ω"
        """
        try:
            text = soup.get_text()
            
            # –ú–ï–¢–û–î 1: –ò—â–µ–º —Ç–µ–∫—Å—Ç "–í—Ä–µ–º—è —á—Ç–µ–Ω–∏—è:"
            match = re.search(r'–í—Ä–µ–º—è\s+—á—Ç–µ–Ω–∏—è[:\s]+(\d+)\s*–º–∏–Ω', text, re.IGNORECASE)
            if match:
                return int(match.group(1))
            
            # –ú–ï–¢–û–î 2: –ò—â–µ–º —á–µ—Ä–µ–∑ meta —Ç–µ–≥–∏
            meta_time = soup.find('meta', {'property': 'article:reading_time'})
            if meta_time:
                time_str = meta_time.get('content', '')
                numbers = re.findall(r'\d+', time_str)
                if numbers:
                    return int(numbers[0])
            
            # –ú–ï–¢–û–î 3: –ò—â–µ–º –≤ —Ä–∞–∑–Ω—ã—Ö data –∞—Ç—Ä–∏–±—É—Ç–∞—Ö
            for elem in soup.find_all(['div', 'span']):
                if elem.get('data-read-time'):
                    times = re.findall(r'\d+', str(elem.get('data-read-time')))
                    if times:
                        return int(times[0])
                
                classes = elem.get('class', [])
                if any('time' in c.lower() or 'read' in c.lower() for c in classes if isinstance(c, str)):
                    if '–º–∏–Ω' in elem.get_text().lower():
                        numbers = re.findall(r'\d+', elem.get_text())
                        if numbers:
                            return int(numbers[0])
            
            # –ú–ï–¢–û–î 4: –ò—Å—Ö–æ–¥–Ω—ã–π –º–µ—Ç–æ–¥ - —á–µ—Ä–µ–∑ pt-3 —Å–µ–ª–µ–∫—Ç–æ—Ä (—á–µ—Ç–≤–µ—Ä—Ç–∞—è –ø–æ–∑–∏—Ü–∏—è)
            section = soup.find('section', class_='section-purple')
            if section:
                divs = section.find_all('div', class_='pt-3')
                if divs:
                    pt3_div = divs[0]
                    child_divs = pt3_div.find_all('div', recursive=False)
                    if len(child_divs) >= 4:  # 4-—è –ø–æ–∑–∏—Ü–∏—è –¥–ª—è –≤—Ä–µ–º–µ–Ω–∏
                        time_text = child_divs[3].get_text(strip=True)
                        numbers = re.findall(r'\d+', time_text)
                        if numbers:
                            return int(numbers[0])
            
            return '–ù–µ —É–∫–∞–∑–∞–Ω–æ'
        
        except Exception as e:
            return '–ù–µ —É–∫–∞–∑–∞–Ω–æ'
    
    def scrape(self, use_file=True, max_articles=None):
        """–ó–∞–ø—É—Å—Ç–∏—Ç—å –ø—Ä–æ—Ü–µ—Å—Å –ø–∞—Ä—Å–∏–Ω–≥–∞ —Å—Ç–∞—Ç–µ–π"""
        print("üîÑ –ó–∞–ø—É—Å–∫ –ø–∞—Ä—Å–∏–Ω–≥–∞ 1skin.ru...\n")
        
        if use_file:
            urls = self.load_urls_from_file()
        else:
            urls = self.get_sitemap_urls()
        
        if not urls:
            print("‚úó –ù–µ —É–¥–∞–ª–æ—Å—å –ø–æ–ª—É—á–∏—Ç—å URL")
            return []
        
        if max_articles:
            urls = urls[:max_articles]
        
        print(f"üìÑ –ü–∞—Ä—Å–∏–Ω–≥ {len(urls)} —Å—Ç–∞—Ç–µ–π...\n")
        
        success_count = 0
        
        for i, url in enumerate(urls, 1):
            article_name = url.split('/')[-2]
            print(f"[{i}/{len(urls)}] {article_name}...", end=' ', flush=True)
            
            data = self.extract_article_data(url)
            
            if data:
                self.articles.append(data)
                print(f"‚úì {data['–î–∞—Ç–∞ –ø—É–±–ª–∏–∫–∞—Ü–∏–∏']} | UPD: {data['–î–∞—Ç–∞ –æ–±–Ω–æ–≤–ª–µ–Ω–∏—è']} | {data['–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –ø—Ä–æ—Å–º–æ—Ç—Ä–æ–≤']} v | {data['–í—Ä–µ–º—è —á—Ç–µ–Ω–∏—è (–º–∏–Ω)']} –º–∏–Ω")
                success_count += 1
            else:
                print("‚úó")
            
            time.sleep(1)
        
        print(f"\n‚úì –£—Å–ø–µ—à–Ω–æ —Å–æ–±—Ä–∞–Ω–æ {success_count}/{len(urls)} —Å—Ç–∞—Ç–µ–π")
        return self.articles
    
    def save_to_excel(self, filename='1skin_articles.xlsx'):
        """–°–æ—Ö—Ä–∞–Ω–∏—Ç—å —Å–ø–∞—Ä—Å–µ–Ω–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ –≤ —Ñ–∞–π–ª Excel"""
        
        if not self.articles:
            print("‚úó –ù–µ—Ç –¥–∞–Ω–Ω—ã—Ö –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è")
            return False
        
        try:
            df = pd.DataFrame(self.articles)
            
            # –ò—Å–ø–æ–ª—å–∑—É–µ–º —Ç–µ–∫—É—â—É—é –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—é –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è
            output_path = Path.cwd() / filename
            
            print(f"\nüíæ –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –≤: {output_path}")
            
            # –ü—ã—Ç–∞–µ–º—Å—è —Å–æ—Ö—Ä–∞–Ω–∏—Ç—å —Å openpyxl
            try:
                with pd.ExcelWriter(output_path, engine='openpyxl') as writer:
                    df.to_excel(writer, index=False, sheet_name='–°—Ç–∞—Ç—å–∏')
                    
                    worksheet = writer.sheets['–°—Ç–∞—Ç—å–∏']
                    worksheet.column_dimensions['A'].width = 50  # URL
                    worksheet.column_dimensions['B'].width = 15  # –î–∞—Ç–∞ –ø—É–±–ª–∏–∫–∞—Ü–∏–∏
                    worksheet.column_dimensions['C'].width = 15  # –î–∞—Ç–∞ –æ–±–Ω–æ–≤–ª–µ–Ω–∏—è
                    worksheet.column_dimensions['D'].width = 20  # –ü—Ä–æ—Å–º–æ—Ç—Ä—ã
                    worksheet.column_dimensions['E'].width = 18  # –í—Ä–µ–º—è —á—Ç–µ–Ω–∏—è
            
            except ImportError:
                # –ï—Å–ª–∏ openpyxl –Ω–µ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω, –∏—Å–ø–æ–ª—å–∑—É–µ–º xlsxwriter
                print("  (openpyxl –Ω–µ –Ω–∞–π–¥–µ–Ω, –∏—Å–ø–æ–ª—å–∑—É—é xlsxwriter)")
                with pd.ExcelWriter(output_path, engine='xlsxwriter') as writer:
                    df.to_excel(writer, index=False, sheet_name='–°—Ç–∞—Ç—å–∏')
            
            except Exception as e:
                # –ï—Å–ª–∏ –æ–±–∞ –Ω–µ —Ä–∞–±–æ—Ç–∞—é—Ç, —Å–æ—Ö—Ä–∞–Ω—è–µ–º –∫–∞–∫ CSV
                print(f"  –û—à–∏–±–∫–∞ —Å Excel: {e}")
                print("  –°–æ—Ö—Ä–∞–Ω—è—é –∫–∞–∫ CSV –≤–º–µ—Å—Ç–æ —ç—Ç–æ–≥–æ...")
                csv_path = Path.cwd() / filename.replace('.xlsx', '.csv')
                df.to_csv(csv_path, index=False, encoding='utf-8-sig')
                output_path = csv_path
            
            print(f"‚úÖ –§–∞–π–ª —Å–æ—Ö—Ä–∞–Ω–µ–Ω: {output_path}")
            print(f"   –†–∞–∑–º–µ—Ä: {output_path.stat().st_size / 1024:.1f} KB")
            
            print(f"\nüìä –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞:")
            print(f"  ‚Ä¢ –í—Å–µ–≥–æ —Å—Ç—Ä–æ–∫: {len(df)}")
            
            # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –ø–æ –ø—Ä–æ—Å–º–æ—Ç—Ä–∞–º
            numeric_views = pd.to_numeric(df['–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –ø—Ä–æ—Å–º–æ—Ç—Ä–æ–≤'], errors='coerce')
            if numeric_views.notna().any():
                avg_views = numeric_views.mean()
                max_views = numeric_views.max()
                min_views = numeric_views.min()
                print(f"  ‚Ä¢ –°—Ä–µ–¥–Ω–µ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –ø—Ä–æ—Å–º–æ—Ç—Ä–æ–≤: {avg_views:.0f}")
                print(f"  ‚Ä¢ –ú–∞–∫—Å –ø—Ä–æ—Å–º–æ—Ç—Ä–æ–≤: {max_views:.0f}")
                print(f"  ‚Ä¢ –ú–∏–Ω –ø—Ä–æ—Å–º–æ—Ç—Ä–æ–≤: {min_views:.0f}")
            
            # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –ø–æ –≤—Ä–µ–º–µ–Ω–∏ —á—Ç–µ–Ω–∏—è
            numeric_time = pd.to_numeric(df['–í—Ä–µ–º—è —á—Ç–µ–Ω–∏—è (–º–∏–Ω)'], errors='coerce')
            if numeric_time.notna().any():
                avg_time = numeric_time.mean()
                print(f"  ‚Ä¢ –°—Ä–µ–¥–Ω–µ–µ –≤—Ä–µ–º—è —á—Ç–µ–Ω–∏—è: {avg_time:.1f} –º–∏–Ω")
            
            # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –ø–æ –¥–∞—Ç–∞–º –æ–±–Ω–æ–≤–ª–µ–Ω–∏—è
            updated_articles = df[df['–î–∞—Ç–∞ –æ–±–Ω–æ–≤–ª–µ–Ω–∏—è'] != '–ù–µ —É–∫–∞–∑–∞–Ω–∞'].shape[0]
            print(f"  ‚Ä¢ –°—Ç–∞—Ç–µ–π —Å –¥–∞—Ç–æ–π –æ–±–Ω–æ–≤–ª–µ–Ω–∏—è: {updated_articles}/{len(df)}")
            
            print(f"\nüìã –ü—Ä–∏–º–µ—Ä—ã –¥–∞–Ω–Ω—ã—Ö (–ø–µ—Ä–≤—ã–µ 3 —Å—Ç—Ä–æ–∫–∏):")
            print(df.head(3).to_string(index=False))
            
            return True
        
        except Exception as e:
            print(f"‚úó –û—à–∏–±–∫–∞ –ø—Ä–∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–∏ —Ñ–∞–π–ª–∞: {e}")
            print(f"  –ü–æ–ª–Ω–∞—è –æ—à–∏–±–∫–∞: {type(e).__name__}: {str(e)}")
            return False


# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
# –ì–õ–ê–í–ù–ê–Ø –ü–†–û–ì–†–ê–ú–ú–ê
# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê

if __name__ == "__main__":
    print("‚ïê" * 80)
    print("1SKIN PARSER - URL ARTICLE SCRAPER v2.1")
    print("‚ïê" * 80 + "\n")
    
    scraper = SkinArticleScraper(urls_file='urls.txt')
    articles = scraper.scrape(use_file=True, max_articles=None)
    
    if articles:
        scraper.save_to_excel('1skin_articles.xlsx')
    else:
        print("‚úó –ù–µ –±—ã–ª–æ —Å–æ–±—Ä–∞–Ω–æ –Ω–∏–∫–∞–∫–∏—Ö –¥–∞–Ω–Ω—ã—Ö")
    
    print("\n‚úÖ –ü–∞—Ä—Å–∏–Ω–≥ –∑–∞–≤–µ—Ä—à–µ–Ω!")
